{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Mon Jan  8 2020\n",
    "\n",
    "@author: Or Duek\n",
    "Check Aging data timeseries\n",
    "\"\"\"\n",
    "\n",
    "# Aging timeseries analysis\n",
    "# Using the connUtils.py file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/or/miniconda3/envs/neuroAnalysis/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from nilearn import plotting\n",
    "import numpy as np\n",
    "from generalUtils import removeVars, timeSeriesSingle, createCorMat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we load the atlas - this is the Yeo one, but we can use others\n",
    "#atlas_filename = '/home/or/Downloads/1000subjects_reference_Yeo/Yeo_JNeurophysiol11_SplitLabels/MNI152/Yeo2011_17Networks_N1000.split_components.FSL_MNI152_1mm.nii.gz'\n",
    "#atlas_labes = pd.read_csv('/home/or/Downloads/1000subjects_reference_Yeo/Yeo_JNeurophysiol11_SplitLabels/Yeo2011_17networks_N1000.split_components.glossary.csv')\n",
    "#coords = coords = plotting.find_parcellation_cut_coords(labels_img=atlas_filename)\n",
    "# take one subjects file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use aal atlas\n",
    "import nilearn\n",
    "aal_atlas = nilearn.datasets.fetch_atlas_aal(version='SPM12', data_dir=None, url=None, resume=True, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "atlas_filename = aal_atlas.maps\n",
    "atlas_labels = aal_atlas.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atlas_labels\n",
    "atlas_labels.index('Occipital_Sup_L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put functional file, confound file and event file here - this is for one subject\n",
    "func_file = '/media/Data/Aging/Preprocessed_Data/aging_output/fmriprep/sub-010/ses-1/func/sub-010_ses-1_task-taska_space-MNI152NLin6Asym_desc-preproc_bold.nii.gz'\n",
    "confound_file = '/media/Data/Aging/Preprocessed_Data/aging_output/fmriprep/sub-010/ses-1/func/sub-010_ses-1_task-taska_desc-confounds_regressors.tsv'\n",
    "events_file = '/media/Data/work/AgingGLM/event_files/sub-010_taska.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NiftiLabelsMasker.fit_transform] loading data from /home/or/nilearn_data/aal_SPM12/aal/atlas/AAL.nii\n",
      "Resampling labels\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling nilearn.input_data.base_masker.filter_and_extract...\n",
      "filter_and_extract('/media/Data/Aging/Preprocessed_Data/aging_output/fmriprep/sub-010/ses-1/func/sub-010_ses-1_task-taska_space-MNI152NLin6Asym_desc-preproc_bold.nii.gz', \n",
      "<nilearn.input_data.nifti_labels_masker._ExtractionFunctor object at 0x7fc709362e90>, \n",
      "{ 'background_label': 0,\n",
      "  'detrend': False,\n",
      "  'dtype': None,\n",
      "  'high_pass': 0.01,\n",
      "  'labels_img': '/home/or/nilearn_data/aal_SPM12/aal/atlas/AAL.nii',\n",
      "  'low_pass': 0.1,\n",
      "  'mask_img': None,\n",
      "  'smoothing_fwhm': 6,\n",
      "  'standardize': True,\n",
      "  't_r': 1,\n",
      "  'target_affine': None,\n",
      "  'target_shape': None}, confounds=array([[6.905034e+03, ..., 2.179170e-04],\n",
      "       ...,\n",
      "       [6.677441e+03, ..., 1.568880e-04]]), dtype=None, memory=Memory(cachedir='nilearn_cashe/joblib'), memory_level=1, verbose=5)\n",
      "[NiftiLabelsMasker.transform_single_imgs] Loading data from /media/Data/Aging/Preprocessed_Data/aging_output/fmriprep/sub-010/ses-1/func/sub-010_ses-1_task-taska_space-MNI152NLin6Asym_desc-preproc_bold.nii.gz\n",
      "[NiftiLabelsMasker.transform_single_imgs] Smoothing images\n",
      "[NiftiLabelsMasker.transform_single_imgs] Extracting region signals\n",
      "[NiftiLabelsMasker.transform_single_imgs] Cleaning extracted signals\n",
      "______________________________________________filter_and_extract - 32.1s, 0.5min\n"
     ]
    }
   ],
   "source": [
    "# create timeseries of all ROIs in atlas\n",
    "timeSer= timeSeriesSingle(func_file, confound_file, atlas_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeSer.shape\n",
    "# 116 regions in aal atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def onsetDuration(onsets, durations, timeseries):\n",
    "# this function receives array of onsets and duration of the same trial type and retures timeseries of this trialtype\n",
    "    addDur = 8 # addint number of TRs to get the HRF\n",
    "    beforeOnset = 3 # how many TRs before onset, to see full HRF\n",
    "    newlist = []\n",
    "    for onset, duration in zip(onsets, durations):\n",
    "        \n",
    "        newlist.append(timeseries[onset-beforeOnset:onset+duration+addDur, :]) \n",
    "    return newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratifyTimeseries (events_file, subject_timeseries, subject_id, trial_line):\n",
    "    #trial_line is a parameter - if 0 then will create each line as file. If 1 then each task\n",
    "    # grab subject events file\n",
    "    events = pd.read_csv(events_file, sep=r'\\s+')\n",
    "    timeSeries = subject_timeseries#np.array(np.load(subject_timeseries, allow_pickle = True))\n",
    "    \n",
    "    # read line  by line and create matrix per line\n",
    "    if trial_line==0:\n",
    "        for line in events.iterrows():    \n",
    "            numberRow = line[0] # take row number to add to matrix name later\n",
    "            onset = round(line[1].onset) # take onset and round it\n",
    "            duration = round(line[1].duration)\n",
    "            trial_type = line[1].trial_type\n",
    "            specTimeline = timeSeries[onset:(onset+duration),:]\n",
    "    \n",
    "    elif trial_line==1: # read by trial type and create specific timeline for each script\n",
    "        PicLossOnset = []\n",
    "        PicGainOnset = []\n",
    "        MoneyLossOnset = []\n",
    "        MoneyGainOnset = []\n",
    "        PicLossDuration = []\n",
    "        PicGainDuration = []\n",
    "        MoneyLossDuration = []\n",
    "        MoneyGainDuration = []\n",
    "        for line in events.iterrows(): # runs trhough the events file, takes the specific files and create timeseries per each\n",
    "            if line[1]['trial_type'].find('Mon_loss')!= -1:\n",
    "              #  print('Mon_loss')\n",
    "                MoneyLossOnset.append(round(line[1].onset))\n",
    "                MoneyLossDuration.append(round(line[1].duration))\n",
    "            elif line[1]['trial_type'].find('Mon_gain')!= -1:\n",
    "               # print('Mon_gain')\n",
    "                MoneyGainOnset.append(round(line[1].onset))\n",
    "                MoneyGainDuration.append(round(line[1].duration))\n",
    "            elif line[1]['trial_type'].find('Pic_loss')!= -1:\n",
    "                #print('Pic_loss')\n",
    "                PicLossOnset.append(round(line[1].onset))\n",
    "                PicLossDuration.append(round(line[1].duration))\n",
    "            elif line[1]['trial_type'].find('Pic_gain')!= -1:\n",
    "               # print('Pic_gain')\n",
    "                PicGainOnset.append(round(line[1].onset))\n",
    "                PicGainDuration.append(round(line[1].duration))\n",
    "        MoneyLoss_timeline = onsetDuration(MoneyLossOnset, MoneyLossDuration, timeSeries)\n",
    "        MoneyGain_timeline = onsetDuration(MoneyGainOnset, MoneyGainDuration, timeSeries)\n",
    "        PicLoss_timeline = onsetDuration(PicLossOnset, PicLossDuration, timeSeries)\n",
    "        PicGain_timeline = onsetDuration(PicGainOnset, PicGainDuration, timeSeries)\n",
    "    \n",
    "        return MoneyGain_timeline, MoneyLoss_timeline, PicGain_timeline, PicLoss_timeline\n",
    "   \n",
    "    else:\n",
    "        print (\"Need to run by task\")\n",
    "    # extract subject specific timeseries from 3D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moneyGaintime, MoneyLossTime, PicGainTime, PicLossTime = stratifyTimeseries(events_file, timeSer, '10', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def averaging_response(taskType_array):\n",
    "    # this function receives taskType array and return averaged response\n",
    "    average = np.mean(taskType_array, axis = 0)\n",
    "    return np.array(average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we plot the average of one task (adding N TRs to the real duration to get full HRF)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(averaging_response(MoneyLossTime)[:,48])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " pd.read_csv(events_file, sep=r'\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create full TRs with average response per trial\n",
    "def overlay_average(timeSer, trial_type, events_file, task_array, region_num):\n",
    "    a = np.zeros(timeSer.shape[0])\n",
    "    trial = trial_type\n",
    "    events = pd.read_csv(events_file, sep=r'\\s+')\n",
    "    onsets_of = np.array(events[events.trial_type== trial].onset)\n",
    "    for i in onsets_of:\n",
    "        print (i)\n",
    "        a[i: i + np.array(task_array).shape[1]] = averaging_response(task_array)[:,region_num]\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(PicGainTime).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_num = 48\n",
    "moneygain_overlay = overlay_average(timeSer, 'Mon_gain', events_file, moneyGaintime, region_num)\n",
    "moneyloss_overlay = overlay_average(timeSer, 'Mon_loss', events_file, MoneyLossTime, region_num)\n",
    "picloss_overlay = overlay_average(timeSer, 'Pic_loss', events_file, PicLossTime, region_num)\n",
    "picGain_overlay = overlay_average(timeSer, 'Pic_gain', events_file, PicGainTime, region_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = [40,10])\n",
    "plt.plot(timeSer[:,43]) # time series of region 43 (V1)\n",
    "plt.plot(moneygain_overlay[:])\n",
    "plt.plot(moneyloss_overlay[:])\n",
    "plt.plot(picGain_overlay[:])\n",
    "plt.plot(picloss_overlay[:])\n",
    "plt.title('Time series')\n",
    "plt.xlabel('Scan number')\n",
    "plt.ylabel('Normalized signal')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
